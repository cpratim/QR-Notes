% --- LaTeX Lecture Notes Template - S. Venkatraman ---

% --- Set document class and font size ---

\documentclass[letterpaper, 11pt]{article}

% --- Package imports ---

% Extended set of colors
\usepackage[dvipsnames]{xcolor}

\usepackage{
  amsmath, amsthm, amssymb, mathtools, dsfont, units,          % Math typesetting
  graphicx, wrapfig, subfig, float,                            % Figures and graphics formatting
  listings, color, inconsolata, pythonhighlight,               % Code formatting
  fancyhdr, sectsty, hyperref, enumerate, enumitem, framed}    % Headers/footers, section fonts, links, lists

\usepackage{minted}
% lipsum is just for generating placeholder text and can be removed
\usepackage{hyperref, lipsum} 

% --- Fonts ---

\usepackage{newpxtext, newpxmath, inconsolata}

% --- Page layout settings ---

% Set page margins
\usepackage[left=.75in, right=.75in, top=.75in, bottom=.75in, headsep=.2in, footskip=0.35in]{geometry}

% Anchor footnotes to the bottom of the page
\usepackage[bottom]{footmisc}

% Set line spacing
\renewcommand{\baselinestretch}{1.2}

% Set spacing between paragraphs
\setlength{\parskip}{1.3mm}

% Allow multi-line equations to break onto the next page
\allowdisplaybreaks

% --- Page formatting settings ---

% Set image captions to be italicized
\usepackage[font={it,footnotesize}]{caption}

% Set link colors for labeled items (blue), citations (red), URLs (orange)
\hypersetup{colorlinks=true, linkcolor=RoyalBlue, citecolor=RedOrange, urlcolor=ForestGreen}

% Set font size for section titles (\large) and subtitles (\normalsize) 
\usepackage{titlesec}
\titleformat{\section}{\large\bfseries}{{\fontsize{19}{19}\selectfont\textreferencemark}\;\; }{0em}{}
\titleformat{\subsection}{\normalsize\bfseries\selectfont}{\thesubsection\;\;\;}{0em}{}

% Enumerated/bulleted lists: make numbers/bullets flush left
%\setlist[enumerate]{wide=2pt, leftmargin=16pt, labelwidth=0pt}
\setlist[itemize]{wide=0pt, leftmargin=16pt, labelwidth=10pt, align=left}

% --- Table of contents settings ---

\usepackage[subfigure]{tocloft}

% Reduce spacing between sections in table of contents
\setlength{\cftbeforesecskip}{.9ex}

% Remove indentation for sections
\cftsetindents{section}{0em}{0em}

% Set font size (\large) for table of contents title
\renewcommand{\cfttoctitlefont}{\large\bfseries}

% Remove numbers/bullets from section titles in table of contents
\makeatletter
\renewcommand{\cftsecpresnum}{\begin{lrbox}{\@tempboxa}}
\renewcommand{\cftsecaftersnum}{\end{lrbox}}
\makeatother

% --- Set path for images ---

\graphicspath{{Images/}{../Images/}}

% --- Math/Statistics commands ---

% Add a reference number to a single line of a multi-line equation
% Usage: "\numberthis\label{labelNameHere}" in an align or gather environment
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

% Shortcut for bold text in math mode, e.g. $\b{X}$
\let\b\mathbf

% Shortcut for bold Greek letters, e.g. $\bg{\beta}$
\let\bg\boldsymbol

% Shortcut for calligraphic script, e.g. %\mc{M}$
\let\mc\mathcal

% \mathscr{(letter here)} is sometimes used to denote vector spaces
\usepackage[mathscr]{euscript}

% Convergence: right arrow with optional text on top
% E.g. $\converge[p]$ for converges in probability
\newcommand{\converge}[1][]{\xrightarrow{#1}}

% Weak convergence: harpoon symbol with optional text on top
% E.g. $\wconverge[n\to\infty]$
\newcommand{\wconverge}[1][]{\stackrel{#1}{\rightharpoonup}}

% Equality: equals sign with optional text on top
% E.g. $X \equals[d] Y$ for equality in distribution
\newcommand{\equals}[1][]{\stackrel{\smash{#1}}{=}}

% Normal distribution: arguments are the mean and variance
% E.g. $\normal{\mu}{\sigma}$
\newcommand{\normal}[2]{\mathcal{N}\left(#1,#2\right)}

% Uniform distribution: arguments are the left and right endpoints
% E.g. $\unif{0}{1}$
\newcommand{\unif}[2]{\text{Uniform}(#1,#2)}

% Independent and identically distributed random variables
% E.g. $ X_1,...,X_n \iid \normal{0}{1}$
\newcommand{\iid}{\stackrel{\smash{\text{iid}}}{\sim}}

% Sequences (this shortcut is mostly to reduce finger strain for small hands)
% E.g. to write $\{A_n\}_{n\geq 1}$, do $\bk{A_n}{n\geq 1}$
\newcommand{\bk}[2]{\{#1\}_{#2}}

% Math mode symbols for common sets and spaces. Example usage: $\R$
\newcommand{\R}{\mathbb{R}}	% Real numbers
\newcommand{\C}{\mathbb{C}}	% Complex numbers
\newcommand{\Q}{\mathbb{Q}}	% Rational numbers
\newcommand{\Z}{\mathbb{Z}}	% Integers
\newcommand{\N}{\mathbb{N}}	% Natural numbers
\newcommand{\F}{\mathcal{F}}	% Calligraphic F for a sigma algebra
\newcommand{\El}{\mathcal{L}}	% Calligraphic L, e.g. for L^p spaces

% Math mode symbols for probability
\newcommand{\pr}{\mathbb{P}}	% Probability measure
\newcommand{\E}{\mathbb{E}}	% Expectation, e.g. $\E(X)$
\newcommand{\var}{\text{Var}}	% Variance, e.g. $\var(X)$
\newcommand{\cov}{\text{Cov}}	% Covariance, e.g. $\cov(X,Y)$
\newcommand{\corr}{\text{Corr}}	% Correlation, e.g. $\corr(X,Y)$
\newcommand{\B}{\mathcal{B}}	% Borel sigma-algebra

% Other miscellaneous symbols
\newcommand{\tth}{\text{th}}	% Non-italicized 'th', e.g. $n^\tth$
\newcommand{\Oh}{\mathcal{O}}	% Big-O notation, e.g. $\O(n)$
\newcommand{\1}{\mathds{1}}	% Indicator function, e.g. $\1_A$

% Additional commands for math mode
\DeclareMathOperator*{\argmax}{argmax}		% Argmax, e.g. $\argmax_{x\in[0,1]} f(x)$
\DeclareMathOperator*{\argmin}{argmin}		% Argmin, e.g. $\argmin_{x\in[0,1]} f(x)$
\DeclareMathOperator*{\spann}{Span}		% Span, e.g. $\spann\{X_1,...,X_n\}$
\DeclareMathOperator*{\bias}{Bias}		% Bias, e.g. $\bias(\hat\theta)$
\DeclareMathOperator*{\ran}{ran}			% Range of an operator, e.g. $\ran(T) 
\DeclareMathOperator*{\dv}{d\!}			% Non-italicized 'with respect to', e.g. $\int f(x) \dv x$
\DeclareMathOperator*{\diag}{diag}		% Diagonal of a matrix, e.g. $\diag(M)$
\DeclareMathOperator*{\trace}{trace}		% Trace of a matrix, e.g. $\trace(M)$
\DeclareMathOperator*{\supp}{supp}		% Support of a function, e.g., $\supp(f)$

% Numbered theorem, lemma, etc. settings - e.g., a definition, lemma, and theorem appearing in that 
% order in Lecture 2 will be numbered Definition 2.1, Lemma 2.2, Theorem 2.3. 
% Example usage: \begin{theorem}[Name of theorem] Theorem statement \end{theorem}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}

% Un-numbered theorem, lemma, etc. settings
% Example usage: \begin{lemma*}[Name of lemma] Lemma statement \end{lemma*}
\newtheorem*{theorem*}{Theorem}
\newtheorem*{proposition*}{Proposition}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{corollary*}{Corollary}
\newtheorem*{definition*}{Definition}
\newtheorem*{example*}{Example}
\newtheorem*{remark*}{Remark}
\newtheorem*{claim}{Claim}

% --- Left/right header text (to appear on every page) ---

% Do not include a line under header or above footer
\pagestyle{fancy}
\renewcommand{\footrulewidth}{0pt}
\renewcommand{\headrulewidth}{0pt}

% Right header text: Lecture number and title
\renewcommand{\sectionmark}[1]{\markright{#1} }
\fancyhead[R]{\small\textit{\nouppercase{\rightmark}}}

% Left header text: Short course title, hyperlinked to table of contents
\fancyhead[L]{\hyperref[sec:contents]{\small Quantitative Research Notes}}

% --- Document starts here ---

\begin{document}

% --- Main title and subtitle ---

\title{Quantitative Research Notes \\[1em]
\normalsize A short complation of knowledge }

% --- Author and date of last update ---
\author{\normalsize Pratim Chowdhary}
\date{\normalsize\vspace{-1ex} Last updated: \today}

% --- Add title and table of contents ---

\maketitle
\tableofcontents\label{sec:contents}

% --- Main content: import lectures as subfiles ---

\section{Introduction}

This is a short compilation of notes on various topics in quantitative research. The notes are based on various sources and are intended to be a quick reference guide for students 
as well as for myself to brush up on concepts.

\section{Decision Theory}
\subsection{Framework}
\begin{enumerate}
    \item A \textbf{statistical model} is a framily of distributions, formally defined as:
    \begin{align}
        \mc{P} = \{P_{\theta} : \theta \in \Theta\}
    \end{align}
    where $\Theta$ is the parameter space and $P_{\theta}$ is the distribution of the data given $\theta$. The parameter space $\Theta$ is the set of all possible values of $\theta$.
    \item A \textbf{decision procedure} is a function $\delta : \mc{X} \to \Theta$ that maps the data space $\mc{X}$ to the parameter space $\Theta$.
    For example if we take the example of a weighted coin flips, we have:
    \begin{align}
        \mc{X} = \{0,1\}^n \quad \text{and} \quad \Theta = [0,1]
    \end{align}
    If we are interested in estimating the parameter $\theta$ of the coin, we can define a decision 
    space as $\Theta = [0,1]$ and a decision procedure as,
    \begin{align}
        \delta(x) = \frac{1}{n} \sum_{i=1}^{n} x_i
    \end{align}
    where $x \in \mc{X}$ is the data and $\delta(x)$ is the estimate of the parameter $\theta$. 
    \item A \textbf{loss function} is a function $L : \Theta \times \Theta \to \R$ that measures the loss incurred by choosing $\theta$ when the true parameter is $\theta'$.
    For example in many situtations we use the squared loss function:
    \begin{align}
        L(\theta, \theta') = (\theta - \theta')^2
    \end{align}
    \item The \textbf{risk} of a decision procedure $\delta$ is the expected loss incurred by the decision procedure:
    \begin{align}
        R(\theta, \delta) = \E_{\theta}[L(\theta, \delta(X))]
    \end{align}
    where $\E_{\theta}$ denotes the expectation with respect to the distribution $P_{\theta}$.
\end{enumerate}

\subsection{Data Reduction}
The idea is that not all data is relevant for making decisions. We can hence 
reduce the data to a smaller set of and not lose any information. 
\begin{enumerate}
    \item A \textbf{statistic} is a function $T : \mc{X} \to \mc{T}$ that maps the data space $\mc{X}$ to a smaller space $\mc{T}$.
    \item A statistic $T$ is \textbf{sufficient} for a parameter $\theta$ if the distribution of the data given the statistic $T$ does not depend on $\theta$. 
    Formally if for all $t$, 
    \begin{align}
        P_{\theta}(X | T = t) = P_{\theta'}(X | T = t) \quad \forall \theta, \theta'
    \end{align}
    \item For any matrix $X \in \R^{n \times p}$, $X^T X$ must be at least positive semi-definite, this is because:
    \begin{align}
        v^T X^T X v = (Xv)^T Xv = ||Xv||^2 \geq 0
    \end{align}
    where $v \in \R^p$.

\end{enumerate}

\section{Linear Algebra}

\subsection{Eigenvalues and Eigenvectors}
\begin{enumerate}
    \item The eigenvalues of a matrix $A \in \R^{n \times n}$ are the roots of the characteristic polynomial:
    \begin{align}
        \text{det}(A - \lambda I) = 0
    \end{align}
    where $\lambda$ is the eigenvalue and $I$ is the identity matrix. The eigenvectors are the vectors $v$ such that:
    \begin{align}
        A v = \lambda v
    \end{align}
    \item The trace of a matrix is the sum of its eigenvalues and the determinant is the product of its eigenvalues.
    \item The eigenvectors of a matrix are orthogonal if the matrix is symmetric.
    \item The eigenvectors of a matrix are linearly independent if the matrix is diagonalizable.
\end{enumerate}

\subsection{(Semi) Positive Definite Matrices}
\begin{enumerate}
    \item A matrix $A \in \R^{n \times n}$ is \textbf{positive semi-definite} if for all $x \in \R^n$:
    \begin{align}
        x^T A x \geq 0 
    \end{align}
    Positive definite matrices are defined similarly with the inequality replaced by a strict inequality.
    \item If a matrix is positive semi-definite, then all its eigenvalues are non-negative and if it is positive definite, then all its eigenvalues are positive.

\end{enumerate}

\subsection{Covariance Matrix}
\begin{enumerate}
    \item The covariance matrix of a random vector $X \in \R^n$ is defined as:
    \begin{align}
        \Sigma = \E[(X - \mu)(X - \mu)^T]
    \end{align}
    where $\mu = \E[X]$ is the mean of the random vector $X$. The covariance matrix is a symmetric positive semi-definite matrix. 
    With a real set of data $X \in \R^{n \times p}$, the empircal covariance matrix is defined as:
    \begin{align}
        \hat{\Sigma} = \frac{1}{n - 1} X^T X \in R^{p \times p}
    \end{align}
    where $X$ is the data matrix with $n$ samples and $p$ features.
    \item Note the covariance matrix can only be full rank if $N \ge p$ and none of the features 
    are linearly dependent, this is because:
    \begin{align}
        \text{rank}(\Sigma) \le \min(n,p)
    \end{align}
    and since $n \ge p$, the rank of the covariance matrix is at most $p$. Some of the 
    useful properties of the covariance matrix are:
    \begin{itemize}
        \item The covariance matrix is symmetric and positive semi-definite.
        \item The covariance matrix is diagonal $\iff$ the features are uncorrelated.
        \item It is related to the correlation matrix by:
        \begin{align}
            \text{Corr}(X) = D^{-1} \Sigma D^{-1} \quad \text{where} \quad D = \text{diag}(\Sigma)^{1/2}
        \end{align}
        \item The covariance matrix is positive definite $\iff$ the features are linearly independent.
    \end{itemize}    
\end{enumerate}

\subsection{Idempotent Matrices}
\begin{enumerate}
    \item A matrix $A \in \R^{n \times n}$ is idempotent if $A^2 = A$. The following are some properties of idempotent matrices:
    \begin{itemize}
        
        \item The eigenvalues of an idempotent matrix are either 0 or 1, this can be seen by:
        \begin{align}
            A v = \lambda v \implies A^2 v = \lambda^2 v \implies \lambda^2 v = \lambda v \implies \lambda = 0,1
        \end{align}
        \item The rank of an idempotent matrix is equal to its trace this is 
        because: since the trace is 
        the sum of the eigenvalues, the rank is the number of non-zero eigenvalues.
        \item The matrix $I - A$ is also idempotent, we can see this by:
        \begin{align}
            (I - A)^2 = I^2 - 2 A + A^2 = I - A
        \end{align}
    \end{itemize}
    \item Any \textbf{projection matrix} is idempotent, this is because the projection matrix projects a vector onto a subspace and then projects it again onto the same subspace.
\end{enumerate}

\subsection{QR Decomposition}
\begin{enumerate}
    \item The \textbf{QR decomposition} of a matrix $A \in \R^{n \times p}$ is a decomposition of the form:
    \begin{align}
        A = Q R \quad \text{where} \quad Q^T Q = I \quad \text{and} \quad R = \begin{bmatrix} r_{ij} \end{bmatrix}
    \end{align}
    where $Q$ is an orthogonal matrix and $R$ is an upper triangular matrix. 
    \begin{align}
        Q = \begin{bmatrix} q_1 & q_2 & \cdots & q_p \end{bmatrix} \quad \text{and} \quad R = \begin{bmatrix} r_{ij} \end{bmatrix}
    \end{align}
    each $q_i$ in $Q$ is an orthogonal vector with $||q_i|| = 1$ and $r_{ij} = 0$ for $i > j$.
    \item The \textbf{Gram-Schmidt} process is a method for computing the QR decomposition of a matrix. The process is as follows:
    \begin{enumerate}
        \item Let $a_1, a_2, \cdots, a_p$ be the columns of the matrix $A$.
        \item Set $q_1 = a_1 / ||a_1||$.
        \item For $i = 2,3,\cdots,p$, set:
        \begin{align}
            q_i = a_i - \sum_{j=1}^{i-1}\text{proj}_{q_j}(a_i) = a_i - \sum_{j=1}^{i-1} \frac{a_i^T q_j}{q_j^T q_j} q_j \rightarrow q_i = \frac{q_i}{||q_i||}
        \end{align}
        the intuition for this is that we are projecting the vector $a_i$ onto the subspace spanned by $q_1, q_2, \cdots, q_{i-1}$ and then subtracting that projection from $a_i$. \\ \\
        Here the $R$ matrix is given by:
        \begin{align}
            R_{ij} = q_i^T a_j
        \end{align}
        as we can get back the original matrix $A$ by:
        \begin{align}
            A = [a_1, a_2, \cdots, a_p] = [q_1, q_2, \cdots, q_p] \begin{bmatrix} q_1^T a_1 & q_1^T a_2 & \cdots & q_1^T a_p \\ 0 & q_2^T a_2 & \cdots & q_2^T a_p \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & q_p^T a_p \end{bmatrix}
        \end{align}
        as $a_i = \sum_{j=1}^{p} q_j R_{ij}$. Intuitively this is true because each $a_i$ is a linear combination of the $q_i$ vectors, specifically
        the linear component is how much of $a_i$ is in the direction of $q_i$ (the projection).
    \end{enumerate}
    \item \textbf{Linear Least Squares}: QR decomposition can help aid in solving the linear least squares problem 
    especially in cases of high collinearity, as:
    \begin{align}
        \text{cond}(X^T X) = \text{cond}(X)^2 \implies (X^T X)^{-1} \quad \text{is ill-conditioned}
    \end{align} 
    Reframing linear least squares with the QR decomposition, we have:
    \begin{align}
        X^T X \hat{\beta} = X^T y \implies R^T Q^T Q R \hat{\beta} = R^T Q^T y \implies R \hat{\beta} = Q^T y
    \end{align}
    Here since $R$ is upper triangular, we can solve for $\hat{\beta}$ by back substitution, 
    creating a more numerically stable solution with easier computation.
     
\end{enumerate}

\subsection{Spectral Theorem}
\begin{enumerate}
    \item The \textbf{spectral theorem} states that any symmetric matrix $A \in \R^{n \times n}$ can be decomposed into the form:
    \begin{align}
        A = Q \Lambda Q^T
    \end{align}
    Where $Q$ is an orthogonal matrix of eigenvectors and $\Lambda$ is a diagonal matrix of eigenvalues.

    \item We first show that if a matrix is symmetric all of its eigenvalues are real. We can show this by considering the eigenvalue equation:
    \begin{align}
        A x = \lambda x \implies x^T A x = x^T \lambda x \implies \lambda = \frac{x^T A x}{x^T x}
    \end{align}
    and since $A$ is symmetric we have that $A = A^T$ and hence:
    \begin{align}
        \lambda = \frac{x^T A x}{x^T x} = \frac{x^T A^T x}{x^T x} = \frac{x^T A x}{x^T x} = \lambda
    \end{align}
    and hence $\lambda$ is real.

    \item We must also show that the eigenvectors are orthogonal. We show this by considering any two eigenvectors $x$ and $y$ of $A$:
    \begin{align}
        A x = \lambda x \quad \text{and} \quad A y = \mu y
    \end{align}
    then we have:
    \begin{align}
        x^T A y = \lambda x^T y \quad \text{and} \quad y^T A x = \mu y^T x
    \end{align}
    and since $A$ is symmetric we have that,
    \begin{align}
        x^T A y = y^T A x \implies \lambda x^T y = \mu y^T x \implies (\lambda - \mu) x^T y = 0
    \end{align}
    and since $x^T y \neq 0$ we have that $\lambda = \mu$.
    \item We can show that $A = Q \Lambda Q^T$ by considering the spectral decomposition of $A$:
    \begin{align}
        Q = \begin{bmatrix} q_1 & q_2 & \cdots & q_n \end{bmatrix} \quad \text{and} \quad \Lambda = \text{diag}(\lambda_1, \lambda_2, \cdots, \lambda_n)
    \end{align}
    We then have that:
    \begin{align}
        A q_i = \lambda_i q_i \implies A Q = Q \Lambda
    \end{align} 
    therefore we have that,
    \begin{align}
        Q \Lambda Q^T = (A) Q Q^T = A
    \end{align}


\end{enumerate}

\subsection{Eigen Decomposition}
\begin{enumerate}
    \item The \textbf{eigen decomposition} of a matrix $A \in \R^{n \times n}$ is a decomposition of the form:
    \begin{align}
        A = Q \Lambda Q^{-1}
    \end{align}
    where $Q$ is the matrix of eigenvectors and $\Lambda$ is the diagonal matrix of eigenvalues.
    \item Any symmetric matrix is diagonalizable, this is because the eigenvectors of a symmetric matrix are orthogonal.
\end{enumerate}

\subsection{Pseudo Inverse}

\section{Optimization}

\section{Probability}
\subsection{Multivariate Normal Distribution}
\begin{enumerate}
    \item A random vector $X \in \R^n$ is said to follow a \textbf{multivariate normal distribution} if it has a joint probability density function of the form:
    \begin{align}
        f(x) = \frac{1}{(2\pi)^{n/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2} (x - \mu)^T \Sigma^{-1} (x - \mu)\right)
    \end{align}
    where $\mu \in \R^n$ is the mean vector and $\Sigma \in \R^{n \times n}$ is the covariance matrix. The covariance matrix $\Sigma$ must be symmetric and positive semi-definite.
\end{enumerate}

\section{Statistics}
\subsection{Hypthothesis Testing}
\begin{enumerate}
    \item A \textbf{hypothesis test} is a statistical test that is used to determine whether there is enough evidence in a sample of data to infer that a certain condition is true for the entire population.
    It usually consists of 
    \begin{itemize}
        \item A null hypothesis $H_0$ that represents a default position that there is no effect or no difference.
        \item A test statistic that is used to determine whether the null hypothesis should be rejected.
        \item A significance level $\alpha$ that determines the probability of rejecting the null hypothesis when it is true.
    \end{itemize}
    \item The \textbf{p-value} is the probability of observing a test statistic as extreme as the one computed from the sample data, assuming that the null hypothesis is true. If the p-value is less than the significance level $\alpha$, then the null hypothesis is rejected.
\end{enumerate}
\section{T-Tests}
\begin{enumerate}
    \item A \textbf{t-test} is a statistical test used to determine if there is a significant difference between the means of two groups. There are different types of t-tests:
    \begin{itemize}
        \item A \textbf{one-sample t-test} is used to compare the mean of a single sample to a known mean.
        \begin{align}
            t = \frac{\bar{x} - \mu}{s / \sqrt{n}} \sim t_{n-1}
        \end{align}
        where $\bar{x}$ is the sample mean, $\mu$ is the known mean, $s$ is the sample standard deviation, and $n$ is the sample size.
        \item A \textbf{two-sample t-test} is used to compare the means of two independent samples.
        \begin{align}
            t = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{s_1^2 / n_1 + s_2^2 / n_2}} \sim t_{n_1 + n_2 - 2}
        \end{align}
        where $\bar{x}_1$ and $\bar{x}_2$ are the sample means, $s_1$ and $s_2$ are the sample standard deviations, and $n_1$ and $n_2$ are the sample sizes.
        \item A \textbf{paired t-test} is used to compare the means of two related samples.
        \begin{align}
            t = \frac{\bar{d}}{s_d / \sqrt{n}} \sim t_{n-1}
        \end{align}
        where $\bar{d}$ is the mean of the differences between the paired samples, $s_d$ is the standard deviation of the differences, and $n$ is the number of pairs.
    \end{itemize}
    after obtaining the t-statistic, the p-value can be computed using the t-distribution and then 
    a decision can be made based on the p-value.
\end{enumerate}
\section{Estimators}
\begin{enumerate}
    \item An \textbf{estimator} is a rule or method for estimating the value of an unknown parameter based on observed data. An estimator is a random variable since it depends on the data.
    \begin{align}
        \hat{\theta} = g(X_1, X_2, \cdots, X_n) \quad \text{where} \quad X_1, X_2, \cdots, X_n \sim F_{\theta}
    \end{align}
    where $F$ is some distribution that depends on the parameter $\theta$.
    \item The \textbf{bias} of an estimator is the difference between the expected value of the estimator and the true value of the parameter being estimated.
    \begin{align}
        \text{Bias}(\hat{\theta}) = \E[\hat{\theta}] - \theta
    \end{align}
    \item An estimator is \textbf{unbiased} if its expected value is equal to the true value of the parameter being estimated, for 
    example if we have,
    \begin{align}
        X_i \sim \text{Normal}(\theta, 1) \quad \text{then} \quad \hat{\theta} = \frac{1}{n} \sum_{i=1}^{n} X_i \quad \text{is unbiased since} \quad \E[\hat{\theta}] = \theta
    \end{align}
    \item An estimator is \textbf{consistent} if it converges in probability to the true value of the parameter being estimated as the sample size increases,
    \begin{align}
        \hat{\theta} \converge[p] \theta \quad \text{as} \quad n \to \infty
    \end{align}
    \item The \textbf{mean squared error} (MSE) of an estimator is the expected value of the squared difference between the estimator and the true value of the parameter being estimated.
    This can be decomposed into the variance of the estimator and the square of the bias:
    \begin{align}
        \text{MSE}(\hat{\theta}) = \E[(\hat{\theta} - \theta)^2] = \text{Var}(\hat{\theta}) + \text{Bias}(\hat{\theta})^2
    \end{align}
    \begin{itemize}
        \item \textbf{Variance} is a measure of how much the estimates for the parameter vary as the sample data changes, 
        for example the error caused by sampling variability.
        \item \textbf{Bias} is a measure of how much the estimates for the parameter differ from the true value of the parameter,
        for example the error caused by using an incorrect or overly simplified model.
    \end{itemize}
    \item \textbf{Maximum Likelihood Estimation}: The maximum likelihood estimator (MLE) is an estimator that maximizes the likelihood function of the observed data, 
    formally:
    \begin{align}
        \hat{\theta}_{\text{MLE}} = \argmax_{\theta} \prod_{i=1}^{n} f_{\theta}(X_i)
    \end{align}
    where $f_{\theta}$ is the probability density function of the data. The MLE is consistent and asymptotically normal.
    \item \textbf{Maximum A Posteriori Estimation}: The maximum a posteriori estimator (MAP) is an estimator that maximizes the posterior distribution of the parameter given the observed data
    and a prior distribution, formally:
    \begin{align}
        \hat{\theta}_{\text{MAP}} = \argmax_{\theta} f(\theta | X) = \argmax_{\theta} f(X | \theta) \pi(\theta)
    \end{align}
    where $f(\theta | X)$ is the posterior distribution, $f(X | \theta)$ is the likelihood function, and $\pi(\theta)$ is the prior distribution. The MAP estimator is consistent and asymptotically normal.

\end{enumerate}

\section{Information Theory}
\subsection{Entropy}
\begin{enumerate}
    \item The \textbf{entropy} of a random variable $X$ is a measure of the uncertainty in the random variable, formally defined as:
    \begin{align}
        H(X) = -\sum_{x} p(x) \log p(x)
    \end{align}
    where $p(x)$ is the probability mass function of the random variable. The entropy is maximized when all outcomes are equally likely (uniform distribution)
\end{enumerate}

% --- Bibliography ---

% Start a bibliography with one item.
% Citation example: "\cite{williams}".

\section{Linear Regression}
\subsection{Assumptions}
\begin{enumerate}
    \item There are 5 key assumptions that are made in linear regression:
    \begin{itemize}
        \item \textbf{Linearity:} The relationship between the dependent and independent variables is linear.
        \item \textbf{Independence:} The residuals are independent of each other,
        \begin{align}
            \text{Cov}(\epsilon_i, \epsilon_j) \approx 0 \quad \forall i \neq j
        \end{align}
        \item \textbf{Homoscedasticity:} The residuals have constant variance (i.e the variance is not a function of the independent variables).
        \item \textbf{Normality:} The residuals are normally distributed,
        \begin{align}
            \epsilon_i \sim \normal{0}{\sigma^2} \quad \text{where} \quad \epsilon_i = y_i - \hat{y}_i
        \end{align}
        \item \textbf{No Multicollinearity:} The independent variables are not highly correlated with each other,
        \begin{align}
            \text{Corr}(X_i, X_j) \approx 0 \quad \forall i \neq j
        \end{align}
        This condition when violated can lead to unstable estimates of the coefficients due to 
        the matrix $X^T X$ being ill-conditioned (high condition number).
    \end{itemize}
\end{enumerate}
\subsection{Ordinary Least Squares}
\begin{enumerate}
    \item \textbf{Problem:} Given a set of data points $\{(x_i, y_i)\}_{i=1}^{n}$, represented by a matrix $X \in \R^{n \times p}$ and a vector $y \in \R^n$, we want to find 
    a linear function $f(x) = x^T \beta$ that minimizes the sum of squared errors:
    \begin{align}
        \argmin_{\beta} ||y - X \beta||^2
    \end{align}
    Note that $X$ is usually augmented with a column of ones to account for the intercept term.
    \item \textbf{Solution:} We can derive the solution to the ordinary least squares problem by setting the gradient of the loss function to zero:
    \begin{align}
        ||y - X \beta||^2 = (X\beta - y)^T(X\beta - y) = \beta^T X^T X \beta - 2 \beta^T X^T y + y^T y \\
        \nabla_{\beta} ||y - X \beta||^2 = 2 X^T X \beta - 2 X^T y = 0 \implies \hat{\beta} = (X^T X)^{-1} X^T y
    \end{align}
    where $\hat{\beta}$ is the least squares estimate of the coefficients.
    \item By the \textbf{Gauss-Markov theorem}, the least squares estimate is the best linear unbiased estimator (BLUE) of the coefficients.
    \item The \textbf{residuals} are the differences between the observed values and the predicted values:
    \begin{align}
        \hat{y} = X \hat{\beta} \quad \text{and} \quad \hat{e}  = y - \hat{y}
    \end{align}
    It can be shown that the residuals are orthogonal to the column space of $X$ (the feature space):
    \begin{align}
        \hat{e} = y - X \hat{\beta} = y - X (X^T X)^{-1} X^T y &\implies X^T \hat{e} = X^T y - X^T X (X^T X)^{-1} X^T y \\
        X^T X (X^T X)^{-1} = I &\implies X^T \hat{e} = X^T y - X^T y = 0
    \end{align}
    This is also intuitive as the residuals are the component of $y$ that is orthogonal to the column space of $X$ (as $\hat{y}$ 
    is the projection of $y$ onto the column space of $X$). \\ \\ 
    It can also be shown that the sum of the residuals is zero:
    \begin{align}
        X^T \hat{e} = 0 \quad \text{and} \quad X^T = [\hat{1} \; |\; X] \implies \hat{1}^T \hat{e} = 0
    \end{align}
    where $\hat{1}$ is the vector of ones that is used to fit the intercept term.
    \item The \textbf{residual sum of squares} (RSS) is the sum of the squared residuals:
    \begin{align}
        \text{RSS} = ||y - X \hat{\beta}||^2 = \hat{e}^T \hat{e}
    \end{align}
    And the \textbf{total sum of squares} (TSS) is the sum of the squared differences between the observed values and the mean of the observed values:
    \begin{align}
        \text{TSS} = ||y - \bar{y} \hat{1}||^2 = (y - \bar{y} \hat{1})^T (y - \bar{y} \hat{1})
    \end{align}
    where $\bar{y}$ is the mean of the observed values. 
    \item The \textbf{coefficient of determination} $R^2$ is the proportion of the variance in the dependent variable that is predictable from the independent variables:
    \begin{align}
        R^2 = 1 - \frac{\text{RSS}}{\text{TSS}} = 1 - \frac{\hat{e}^T \hat{e}}{(y - \bar{y} \hat{1})^T (y - \bar{y} \hat{1})}
    \end{align}
    the $R^2$ value can range from $-\infty$ to $1$ and is a measure of how well the model fits the data, or 
    how much of the variance in the dependent variable is explained by the independent variables.
    \item The least squares estimator for $\beta$ is a \textbf{maximum likelihood estimator} under the assumption that the residuals are normally distributed,
    \begin{align}
        \epsilon_i = y_i - x_i^T \beta \sim \normal{0}{\sigma^2} \quad \text{where} \quad \text{PDF}(\epsilon_i) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{1}{2\sigma^2} \epsilon_i^2\right)
    \end{align}
    therefore we have that:
    \begin{align}
        \text{likelihood}(\beta) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{1}{2\sigma^2} (y_i - x_i^T \beta)^2\right)
    \end{align}
    we maximize by considering the log-likelihood:
    \begin{align}
        \ell(\beta) = -\frac{n}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^{n} (y_i - x_i^T \beta)^2 \propto \min_{\beta} ||y - X \beta||^2
    \end{align}
    which is clearly proportional to minimizing the least squares loss. 
\end{enumerate}
\subsection{Regularization}
\begin{enumerate}
    \item When $X \in \R^{n \times p}$ is not full rank, the matrix $X^T X$ is not invertible and the least squares estimate $\hat{\beta} = (X^T X)^{-1} X^T y$ does not exist.
    this can be seen by the fact that,
    \begin{align}
        \text{rank}(X^T X) \le \min(n,p) \quad \text{and} \quad \text{rank}(X) \le \min(n,p) \quad \text{with} \quad p < n
    \end{align}
    hence the matrix $X^T X$ is not full rank. \\ \\
    Another way $X^T X$ isn't invertible is when $n < p$, in this case the system 
    is underdetermined and there are infinitely many solutions to the least squares problem.
    \item In both of these cases, we can use \textbf{regularization} to stabilize the estimates of the coefficients.
\end{enumerate}
\subsection{Ridge Regression}
\begin{enumerate}
    \item \textbf{Ridge regression} adds a penalty term proportional to the $L_2$ norm of the coefficients to the least squares loss function:
    \begin{align}
        \argmin_{\beta} ||y - X \beta||^2 + \lambda ||\beta||_2^2
    \end{align}
    where $\lambda$ is the regularization parameter that controls the strength of the penalty term.
    \item \textbf{Solution:} We can derive the solution to ridge:
    \begin{align}
        ||y - X \beta||^2 + \lambda ||\beta||_2^2 = (X\beta - y)^T(X\beta - y) + \lambda \beta^T \beta \\
        \nabla_{\beta} ||y - X \beta||^2 + \lambda ||\beta||_2^2 = 2 X^T X \beta - 2 X^T y + 2 \lambda \beta = 0 \\
        \implies \hat{\beta} = (X^T X + \lambda I)^{-1} X^T y
    \end{align}
    where $I$ is the identity matrix. 
    \item \textbf{Rank Analysis:} We can see that if $X^TX$ is not full rank, then matrix $X^TX + \lambda I$ must be full rank. 
    We can see this by considering the eigenvalues of $X^TX$ and $X^TX + \lambda I$:
    \begin{align}
        \alpha \in \text{eig}(X^TX) \implies X^TX v = \alpha v
    \end{align}
    Consider the eigenvalues of $X^TX + \lambda I$:
    \begin{align}
        (X^TX + \lambda I) v = X^TX v + \lambda v = \alpha v + \lambda v = (\alpha + \lambda) v 
    \end{align}
    hence if we have a set of eigenvalues $\alpha$ for $X^TX$, we will have a set of eigenvalues $\alpha + \lambda$ for $X^TX + \lambda I$, 
    and since $X^TX$ is a positive semi-definite matrix, the eigenvalues of $X^TX + \lambda I$ will be strictly positive.
    Therefore we have that,
    \begin{align}
        \det(X^TX) = \prod_{i=1}^{p} \alpha_i \quad \text{and} \quad \det(X^TX + \lambda I) = \prod_{i=1}^{p} (\alpha_i + \lambda)
    \end{align}
    and since the determinant of a matrix is the product of its eigenvalues, we have that the matrix $X^TX + \lambda I$ is full rank. \\ \\ 
    Note that the matrix can still be ill-conditioned if the eigenvalues of $X^TX$ are close to zero, as the eigenvalues of $X^TX + \lambda I$ will be close to $\lambda$, 
    and if $\lambda$ is small then the condition number of the matrix will remain large.
    \item The ridge regression estimate is a \textbf{maximum a posteriori (MAP)} estimate under the 
    assumption that the coefficients are normally distributed with mean zero and variance proportional to $1 / \lambda$:
    \begin{align}
        \beta_i \sim \normal{0}{\lambda} \implies \text{PDF}(\beta_i) = \frac{1}{\sqrt{2\pi \lambda}} \exp\left(-\frac{1}{2\lambda} \beta_i^2\right)
    \end{align}
    using Bayes' theorem, we have that:
    \begin{align}
        \text{likelihood}(\beta) \propto \exp\left(-\frac{1}{2\sigma^2} ||y - X \beta||^2\right) \quad \text{and} \quad \text{prior}(\beta) \propto \exp\left(-\frac{1}{2\lambda} ||\beta||^2\right)
    \end{align}
    therefore we can take the product of the likelihood and the prior to get the posterior:
    \begin{align}
        \text{posterior}(\beta) \propto \exp\left(-\frac{1}{2\sigma^2} ||y - X \beta||^2 - \frac{1}{2\lambda} ||\beta||^2\right)
    \end{align}
    and maximizing the posterior is equivalent to minimizing the ridge regression loss.
\end{enumerate}
\subsection{Lasso Regression}

\section{Trading (Game Theory)}
\subsection{Making Markets}
\begin{enumerate}
    \item When \textbf{making a market} the most important thing to consider is the fair value of whatever
    it is you are trying to price. For example that could be the expectation of a sum of random variables,
    \begin{align}
        \E[X_1 + X_2 + \cdots + X_n] \quad \text{where} \quad X_i \sim \text{Coin Flip}(p)
    \end{align}
    in this case we would be trading on the sum of $n$ coin flips with probability of heads $p$.
    \item The \textbf{bid-ask spread} is the difference between the price at which you 
    are willing to buy and the price at which you are willing to sell. 
    \begin{itemize}
        \item In general for less liquid assets you would want to have a larger spread,
        as the risk of holding the asset is higher.
        \item For higher variance assets you would also want to have a larger spread as the 
        probability that the asset will move against you is higher. For example if we are trading,
        \begin{align}
            X_1 + X_2 + \cdots + X_n \quad \text{where} \quad X_i \sim \text{Normal}(0,1)
        \end{align}
        the variance of the sum is $n$, while in the following case:
        \begin{align}
            X_1 + X_2 + \cdots + X_n \quad \text{where} \quad X_i \sim \text{Normal}(0,0.1)
        \end{align}
        the variance of the sum is $0.1n$, hence the spread should be larger in the first case, even though 
        the expected value of the sum is the same.

    \end{itemize}
    \item \textbf{Impact Function}: When someone trades (typically a large order), on your market 
    it makes sense to adjust your theoretical price, for example if you had a market on $X$,
    \begin{align}
        \text{Bid} \quad  [100] \quad 95.0 \quad - \quad 105.0 \quad [100] \quad \text{Ask} 
    \end{align}
    and someone buys $100$ shares at $105$, you would want to think to yourself, "
    \textit{why did they buy at $105$?}".
    \begin{itemize}
        \item The buyer could have information that moves the fair value up, and they wanted 
        to profit off of that. 
        \item The buyer could just be willing to pay the spread to get the shares quickly to hold for longer
        \item The buyer could be uninformed and just buying because they think the price will go up.
    \end{itemize}
    In the first case (espcially if you think the buyer is informed), you would want to adjust your 
    theoretical price up so that you don't get crushed by future informed traders. \\ \\
    In the other cases you might not want to adjust your price as much, as the buyer might not have
    any information that you don't have and you would be losing money by adjusting your price.
\end{enumerate}

\section{Practical Statistical Learning}

\section{Pandas}
\subsection{DataFrames}
\begin{enumerate}
    \item A DataFrame is a 2-dimensional labeled data structure with columns of potentially different types, some common operations on DataFrames are:
    \begin{itemize}
        \item \textbf{Selecting columns:} Columns can be selected using the column name as an attribute or as a key.
        \item \textbf{Selecting rows:} Rows can be selected using the \texttt{loc} and \texttt{iloc} methods.
        \item \textbf{Filtering rows:} Rows can be filtered using boolean indexing.
        \item \textbf{Applying functions:} Functions can be applied to columns using the \texttt{apply} method.
        \item \textbf{Grouping data:} Data can be grouped using the \texttt{groupby} method.
        \item \textbf{Merging data:} Data can be merged using the \texttt{merge} method.
    \end{itemize}
    Some examples of these operations are:
    \begin{minted}[fontsize=\small, frame=single]{python}
        import pandas as pd
        # Create a sample DataFrame
        data = {
            'name':   ['Alice', 'Bob', 'Charlie', 'Dave', 'Eve'],
            'age':    [24, 42, 18, 68, 32],
            'city':   ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'],
            'salary': [50000, 60000, 45000, 70000, 65000]
        }
        df = pd.DataFrame(data)
        # 1. Selecting columns
        # Using the column name as a key
        names = df['name']
        # Using the column name as an attribute (if it doesn't conflict with method names)
        ages = df.age
        # 2. Selecting rows
        # Using label-based indexing (loc)
        first_row_loc = df.loc[0]      # Select row with label 0
        subset_loc = df.loc[1:3]       # Select rows with labels 1 through 3
        # Using integer-based indexing (iloc)
        first_row_iloc = df.iloc[0]    # Select the first row
        subset_iloc = df.iloc[1:3]     # Select the 2nd and 3rd rows
        # 3. Filtering rows (boolean indexing)
        older_than_30 = df[df['age'] > 30]
        # 4. Applying functions
        # Apply a lambda function to increase salary by 10%
        df['salary_with_bonus'] = df['salary'].apply(lambda x: x * 1.1)
        # 5. Grouping data
        # Calculate the mean salary for each city
        mean_salary_by_city = df.groupby('city')['salary'].mean()
        # 6. Merging data
        # Suppose we have another DataFrame df2 that shares a common key 'name'
        df2 = pd.DataFrame({
            'name':   ['Alice', 'Bob', 'Eve'],
            'bonus':  [5000, 7000, 4000]
        })
        merged_df = pd.merge(df, df2, on='name', how='left')
    \end{minted}
\end{enumerate}

\subsection{Summary Statistics}
\begin{enumerate}
    \item Some common summary statistics for a DataFrame \texttt{df} are:
    \begin{itemize}
        \item \textbf{Descriptive statistics:} The \texttt{describe} method provides summary statistics for numerical columns.
        \item \textbf{Correlation matrix:} The \texttt{corr} method provides the correlation matrix for numerical columns.
        \item \textbf{Unique values:} The \texttt{nunique} method provides the number of unique values for each column.
        \item \textbf{Value counts:} The \texttt{value\_counts} method provides the frequency of each unique value in a column.
        \item \textbf{Missing values:} The \texttt{isnull} method provides a DataFrame of missing values.
    \end{itemize}
    Some examples of these operations are:
    \begin{minted}[fontsize=\small, frame=single]{python}
        # 1. Descriptive statistics
        summary_stats = df.describe()
        # 2. Correlation matrix
        correlation_matrix = df.corr()
        # 3. Unique values
        unique_values = df.nunique()
        # 4. Value counts
        value_counts = df['city'].value_counts()
        # 5. Missing values
        missing_values = df.isnull()
    \end{minted}
\end{enumerate}

\subsection{Data Cleaning}
\begin{enumerate}
    \item A lot of the time data is not clean and therefore needs preprocessing before it can be used for analysis. Some of the basic 
    operations for data cleaning are:
    \begin{itemize}
        \item \textbf{Removing duplicates:} Duplicates can be removed using the \texttt{drop\_duplicates} method.
        \item \textbf{Filling missing values:} Missing values can be filled using the \texttt{fillna} method.
        \item \textbf{Replacing values:} Values can be replaced using the \texttt{replace} method.
        \item \textbf{Changing data types:} Data types can be changed using the \texttt{astype} method.
    \end{itemize}
    Some examples of these operations are:
    \begin{minted}[fontsize=\small, frame=single]{python}
        # 1. Removing duplicates
        df_no_duplicates = df.drop_duplicates()
        # 2. Filling missing values as 0
        df_filled = df.fillna(0)
        # 3. Replacing values
        df_replaced = df.replace('New York', 'NY')
        # 4. Changing data types
        df['age'] = df['age'].astype(float)
    \end{minted}
    \item \textbf{One-Hot Encoding}: for data given in a categorical form, it is often useful to convert it to a numerical form. This can be done using the \texttt{get\_dummies} method.
    \begin{minted}[fontsize=\small, frame=single]{python}
        # Convert the 'city' column to dummy variables
        df_with_dummies = pd.get_dummies(df, columns=['city'])
    \end{minted}
    This will create a new column for each unique value in the 'city' column with a 1 if the value is present and a 0 otherwise.
    \item \textbf{Clipping}: Sometimes it is useful to clip the values of a column to a certain range. This can be done using the \texttt{clip} method.
    \begin{minted}[fontsize=\small, frame=single]{python}
        # Clip the 'age' column to be between 18 and 65
        df_clipped = df['age'].clip(18, 65)
    \end{minted}

\end{enumerate}

\subsection{Time Series}
\begin{enumerate}
    \item Time series data is data that is indexed by time. Some common operations on time series data are:
    \begin{itemize}
        \item \textbf{Resampling:} Time series data can be resampled using the \texttt{resample} method.
        \item \textbf{Shifting:} Time series data can be shifted using the \texttt{shift} method.
        \item \textbf{Rolling windows:} Rolling windows can be applied to time series data using the \texttt{rolling} method.
    \end{itemize}
    Some examples of these operations are:
    \begin{minted}[fontsize=\small, frame=single]{python}
        # 1. Resampling
        # Resample the data to monthly frequency
        df_resampled = df.resample('M').mean()
        # 2. Shifting
        # Shift the data by 1 period (move the data down by 1)
        df_shifted = df.shift(1)
        # 3. Rolling windows
        # Calculate the 7-day rolling mean
        df_rolling = df.rolling(window=7).mean()
    \end{minted}
    
\end{enumerate}

\section{NumPy}
\subsection{Arrays}
\begin{enumerate}
    \item NumPy arrays are the core data structure for numerical computations in Python. Some common operations on NumPy arrays are:
    \begin{itemize}
        \item \textbf{Creating arrays:} Arrays can be created using the \texttt{array} function or using convenience functions like \texttt{zeros}, \texttt{ones}, and \texttt{arange}.
        \item \textbf{Indexing:} Elements of an array can be accessed using square brackets.
        \item \textbf{Slicing:} Subarrays can be accessed using slicing.
        \item \textbf{Reshaping:} Arrays can be reshaped using the \texttt{reshape} method.
        \item \textbf{Stacking:} Arrays can be stacked vertically or horizontally using the \texttt{vstack} and \texttt{hstack} functions.
        \item \textbf{Broadcasting:} Operations can be performed on arrays of different shapes using broadcasting.
    \end{itemize}
    Some examples of these operations are:
    \begin{minted}[fontsize=\small, frame=single]{python}
        import numpy as np
        # 1. Creating arrays
        a = np.array([1, 2, 3])
        b = np.zeros((2, 3))
        c = np.ones((3, 2))
        d = np.arange(10)
        # 2. Indexing
        first_element = a[0]
        # 3. Slicing
        first_two_elements = a[:2]
        # 4. Reshaping
        e = d.reshape((2, 5))
        # 5. Stacking
        f = np.vstack((b, c))
        g = np.hstack((b, c))
        # 6. Broadcasting
        h = a + 1
    \end{minted}
\end{enumerate}

\section{Scikit-Learn}

\begin{thebibliography}{1}

\bibitem{williams}
   Williams, David.
   \textit{Probability with Martingales}.
   Cambridge University Press, 1991.
   Print.

% Uncomment the following lines to include a webpage
% \bibitem{webpage1}
%   LastName, FirstName. ``Webpage Title''.
%   WebsiteName, OrganizationName.
%   Online; accessed Month Date, Year.\\
%   \texttt{www.URLhere.com}

\end{thebibliography}

% --- Document ends here ---

\end{document}
